{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrÃ©chet Inception Distance (FID) Evaluation for Video Frames\n",
    "# This notebook implements FID evaluation for comparing frames from generated videos against frames from real videos.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from scipy import linalg\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Load InceptionV3 Model for Feature Extraction\n",
    "class InceptionV3FeatureExtractor():\n",
    "    def __init__(self, device='cuda'):\n",
    "        # Load pre-trained InceptionV3 model\n",
    "        self.model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "        # Remove the final classification layer\n",
    "        self.model.fc = torch.nn.Identity()\n",
    "        # Set to evaluation mode\n",
    "        self.model.eval()\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Define image preprocessing\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, images):\n",
    "        \"\"\"Extract features from a batch of images\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.model(images)\n",
    "        return features\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    \"\"\"Calculate FID score between real and fake feature distributions\"\"\"\n",
    "    # Calculate mean and covariance for real features\n",
    "    mu1 = np.mean(real_features, axis=0)\n",
    "    sigma1 = np.cov(real_features, rowvar=False)\n",
    "    \n",
    "    # Calculate mean and covariance for fake features\n",
    "    mu2 = np.mean(fake_features, axis=0)\n",
    "    sigma2 = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Calculate FID score\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    # Check if covmean contains complex numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "def calculate_fid_for_frame(real_video_path, gen_video_path, frame_idx=0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate FID score for a specific frame between two videos\n",
    "    \n",
    "    Args:\n",
    "        real_video_path: Path to the real video\n",
    "        gen_video_path: Path to the generated video\n",
    "        frame_idx: Index of the frame to compare (default: 0, first frame)\n",
    "        device: Device to run the model on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        FID score for the specified frame\n",
    "    \"\"\"\n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = InceptionV3FeatureExtractor(device=device)\n",
    "    \n",
    "    # Extract the specified frame from each video\n",
    "    def extract_single_frame(video_path, frame_idx):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            raise ValueError(f\"Could not read frame {frame_idx} from {video_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return frame\n",
    "    \n",
    "    # Get frames\n",
    "    real_frame = extract_single_frame(real_video_path, frame_idx)\n",
    "    gen_frame = extract_single_frame(gen_video_path, frame_idx)\n",
    "    \n",
    "    # Process frames\n",
    "    real_pil = Image.fromarray(real_frame)\n",
    "    gen_pil = Image.fromarray(gen_frame)\n",
    "    \n",
    "    real_tensor = feature_extractor.preprocess(real_pil).unsqueeze(0).to(device)\n",
    "    gen_tensor = feature_extractor.preprocess(gen_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        real_features = feature_extractor.extract_features(real_tensor).cpu().numpy()\n",
    "        gen_features = feature_extractor.extract_features(gen_tensor).cpu().numpy()\n",
    "    \n",
    "    # Calculate FID score\n",
    "    fid_score = calculate_fid(real_features, gen_features)\n",
    "    \n",
    "    return fid_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Non-matrix input to matrix function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m real_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/proj/aicell/users/x_aleho/video-diffusion/data/processed/idr0013/LT0001_02/00001_01.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m gen_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/proj/aicell/users/x_aleho/video-diffusion/CogVideo/test_generations/i2v_eval1_night/LT0004_06-00058_01_withLORA_highPROF.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m fid \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fid_for_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFID score for frame 5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 110\u001b[0m, in \u001b[0;36mcalculate_fid_for_frame\u001b[0;34m(real_video_path, gen_video_path, frame_idx, device)\u001b[0m\n\u001b[1;32m    107\u001b[0m     gen_features \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mextract_features(gen_tensor)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Calculate FID score\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m fid_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fid_score\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[0;34m(real_features, fake_features)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate FID score\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ssdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((mu1 \u001b[38;5;241m-\u001b[39m mu2) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m covmean \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrtm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Check if covmean contains complex numbers\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39miscomplexobj(covmean):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/linalg/_matfuncs_sqrtm.py:167\u001b[0m, in \u001b[0;36msqrtm\u001b[0;34m(A, disp, blocksize)\u001b[0m\n\u001b[1;32m    165\u001b[0m A \u001b[38;5;241m=\u001b[39m _asarray_validated(A, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_inexact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-matrix input to matrix function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blocksize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe blocksize should be at least 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Non-matrix input to matrix function."
     ]
    }
   ],
   "source": [
    "real_video = \"/proj/aicell/users/x_aleho/video-diffusion/data/processed/idr0013/LT0001_02/00001_01.mp4\"\n",
    "gen_video = \"/proj/aicell/users/x_aleho/video-diffusion/CogVideo/test_generations/i2v_eval1_night/LT0004_06-00058_01_withLORA_highPROF.mp4\"\n",
    "fid = calculate_fid_for_frame(real_video, gen_video, frame_idx=5)\n",
    "print(f\"FID score for frame 5: {fid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
